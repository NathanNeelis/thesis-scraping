{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce1c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca79d5",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ddba",
   "metadata": {},
   "source": [
    "## select subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38949ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 44 comments from Pushshift\n"
     ]
    }
   ],
   "source": [
    "subreddit=\"politics\"\n",
    "limit=1000\n",
    "\n",
    "\n",
    "\n",
    "# adding timeframe \n",
    "before = int(dt.datetime(2022,1,2,0,0).timestamp())\n",
    "after = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4abb",
   "metadata": {},
   "source": [
    "## Start scraping\n",
    "Scraping the above selected subreddit for every day. Then saving it in a csv for per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd459ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 1000 comments in year 2022\n",
      "more than 1000 comments in month 1 - 2022\n",
      "datebefore 2 - 1 -  2022 date after 1 - 1 -  2022\n",
      "Retrieved 44 comments per day from Pushshift\n",
      "datebefore 3 - 1 -  2022 date after 2 - 1 -  2022\n",
      "Retrieved 68 comments per day from Pushshift\n",
      "datebefore 4 - 1 -  2022 date after 3 - 1 -  2022\n",
      "Retrieved 76 comments per day from Pushshift\n",
      "datebefore 5 - 1 -  2022 date after 4 - 1 -  2022\n",
      "Retrieved 55 comments per day from Pushshift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datebefore 6 - 1 -  2022 date after 5 - 1 -  2022\n",
      "Retrieved 0 comments per day from Pushshift\n",
      "datebefore 7 - 1 -  2022 date after 6 - 1 -  2022\n",
      "Retrieved 51 comments per day from Pushshift\n",
      "datebefore 8 - 1 -  2022 date after 7 - 1 -  2022\n",
      "Retrieved 34 comments per day from Pushshift\n",
      "datebefore 9 - 1 -  2022 date after 8 - 1 -  2022\n",
      "Retrieved 37 comments per day from Pushshift\n",
      "datebefore 10 - 1 -  2022 date after 9 - 1 -  2022\n",
      "Retrieved 35 comments per day from Pushshift\n",
      "datebefore 11 - 1 -  2022 date after 10 - 1 -  2022\n",
      "Retrieved 54 comments per day from Pushshift\n",
      "datebefore 12 - 1 -  2022 date after 11 - 1 -  2022\n",
      "Retrieved 49 comments per day from Pushshift\n",
      "datebefore 13 - 1 -  2022 date after 12 - 1 -  2022\n",
      "Retrieved 53 comments per day from Pushshift\n",
      "datebefore 14 - 1 -  2022 date after 13 - 1 -  2022\n",
      "Retrieved 62 comments per day from Pushshift\n",
      "datebefore 15 - 1 -  2022 date after 14 - 1 -  2022\n",
      "Retrieved 80 comments per day from Pushshift\n",
      "datebefore 16 - 1 -  2022 date after 15 - 1 -  2022\n",
      "Retrieved 46 comments per day from Pushshift\n",
      "datebefore 17 - 1 -  2022 date after 16 - 1 -  2022\n",
      "Retrieved 50 comments per day from Pushshift\n",
      "datebefore 18 - 1 -  2022 date after 17 - 1 -  2022\n",
      "Retrieved 57 comments per day from Pushshift\n",
      "datebefore 19 - 1 -  2022 date after 18 - 1 -  2022\n",
      "Retrieved 66 comments per day from Pushshift\n",
      "datebefore 20 - 1 -  2022 date after 19 - 1 -  2022\n",
      "Retrieved 78 comments per day from Pushshift\n",
      "datebefore 21 - 1 -  2022 date after 20 - 1 -  2022\n",
      "Retrieved 73 comments per day from Pushshift\n",
      "datebefore 22 - 1 -  2022 date after 21 - 1 -  2022\n",
      "Retrieved 40 comments per day from Pushshift\n",
      "datebefore 23 - 1 -  2022 date after 22 - 1 -  2022\n",
      "Retrieved 38 comments per day from Pushshift\n",
      "datebefore 24 - 1 -  2022 date after 23 - 1 -  2022\n",
      "Retrieved 33 comments per day from Pushshift\n",
      "datebefore 25 - 1 -  2022 date after 24 - 1 -  2022\n",
      "Retrieved 63 comments per day from Pushshift\n",
      "datebefore 26 - 1 -  2022 date after 25 - 1 -  2022\n",
      "Retrieved 35 comments per day from Pushshift\n",
      "datebefore 27 - 1 -  2022 date after 26 - 1 -  2022\n",
      "Retrieved 42 comments per day from Pushshift\n",
      "datebefore 28 - 1 -  2022 date after 27 - 1 -  2022\n",
      "Retrieved 35 comments per day from Pushshift\n",
      "datebefore 29 - 1 -  2022 date after 28 - 1 -  2022\n",
      "Retrieved 62 comments per day from Pushshift\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "for m in range(1, 13):\n",
    "    year = 2022\n",
    "    yearAfter = year\n",
    "    monthBefore = m\n",
    "    monthAfter = m\n",
    "    day = 32\n",
    "    minutes = 0\n",
    "    \n",
    "    # YEARLY SCRAPE\n",
    "    # If year has comments < 1000 scrape everything at once \n",
    "    scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "    scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "       \n",
    "    # Scrape all comments per Year\n",
    "    commentsYear = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "    \n",
    "    \n",
    "    if (len(commentsYear) < 1000): \n",
    "        print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "        # convert comments into a datafrom\n",
    "        comments_df = pd.DataFrame(commentsYear)\n",
    "        \n",
    "        #add all datapoints to a temporary dataframe\n",
    "        temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # MONTLY SCRAPE\n",
    "    else:\n",
    "        print(f'more than {len(commentsYear)} comments in year {year}')\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per month\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            #add all datapoints to a temporary dataframe\n",
    "            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    #add all datapoints to a temporary dataframe\n",
    "                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "                # 12 HOURLY SCRAPE\n",
    "                else:\n",
    "                    print(f'more than {len(commentsDay)} comments on day {dayAfter}-{monthAfter}-{yearAfter} -- {dayBefore}-{monthBefore}-{year}')\n",
    "                    print(f'attempting 12-hourly scrape...')\n",
    "                    for b in range(2):\n",
    "\n",
    "                        hourMeasureBefore = 12\n",
    "                        hourMeasureAfter = 0\n",
    "                        minuteMeasureBefore = 0\n",
    "                        \n",
    "                        if (b == 0):\n",
    "                            hourMeasureBefore = 12\n",
    "                            hourMeasureAfter = 0\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        if (b == 1):\n",
    "                            hourMeasureBefore = 23\n",
    "                            hourMeasureAfter = 12\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        \n",
    "                        scrapeTwelveHourDateBefore = int(dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore).timestamp())\n",
    "                        scrapeTwelveHourDateAfter = int(dt.datetime(year,m,d,hourMeasureAfter,0).timestamp())  \n",
    "\n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsTwelveHour = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeTwelveHourDateBefore, after=scrapeTwelveHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsTwelveHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore), 'date after', dt.datetime(yearAfter,m,d,hourMeasureAfter,0)) \n",
    "                            print(f'Retrieved {len(commentsTwelveHour)} comments per 12 hours from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsTwelveHour)\n",
    "\n",
    "                            #add all datapoints to a temporary dataframe\n",
    "                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                        # HOURLY SCRAPE\n",
    "                        else: \n",
    "                            print(f'more than {len(commentsTwelveHour)} comments: failed 12-hourly scrape, attempting hourly scrape...')\n",
    "                            \n",
    "                            for h in range(24):\n",
    "\n",
    "                                # add an hour to the datetime object\n",
    "                                hourBefore = h + 1\n",
    "                                hourAfter = h\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (hourBefore == 24):\n",
    "                                    hourBefore = 23\n",
    "                                    minutes = 59\n",
    "                                else: \n",
    "                                    hourBefore = h + 1\n",
    "                                    minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                                #set time frame for scraping -> Scraping for every hour in the day\n",
    "                                scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                                scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                                # Scrape all comments per hour\n",
    "                                commentsHour = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsHour) < 1000): \n",
    "                                    print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                                    print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "                                            \n",
    "                                            \n",
    "                                # SCRAPING PER 20 MINUTES\n",
    "                                else:\n",
    "                                    print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "                                \n",
    "                                    for q in range(3):\n",
    "\n",
    "                                        minuteBefore = 20\n",
    "                                        minuteAfter = 0\n",
    "\n",
    "                                        if (q == 0):\n",
    "                                            minuteBefore = 20\n",
    "                                            minuteAfter = 0\n",
    "\n",
    "                                        if (q == 1):\n",
    "                                            minuteBefore = 40\n",
    "                                            minuteAfter = 20\n",
    "                                            \n",
    "                                        if (q == 2):\n",
    "                                            minuteBefore = 59\n",
    "                                            minuteAfter = 40\n",
    "\n",
    "\n",
    "                                        scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                        scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                        # Scrape all comments per 20 mins\n",
    "                                        commentsMinute = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                        # if statement hour comments < 1000\n",
    "                                        if (len(commentsMinute) < 1000): \n",
    "                                            print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minuteBefore)) \n",
    "                                            print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                            # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            #add all datapoints to a temporary dataframe\n",
    "                                            temp_df = temp_df.append(comments_df, ignore_index=True)                                   \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                        else:\n",
    "                                        # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            #add all datapoints to a temporary dataframe\n",
    "                                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "                                            print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                            print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minuteAfter} and {h}:{minuteBefore}')\n",
    "                #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
