{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce1c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca79d5",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ddba",
   "metadata": {},
   "source": [
    "## select subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38949ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 65 comments from Pushshift\n"
     ]
    }
   ],
   "source": [
    "subreddit=\"politics\"\n",
    "limit=1000\n",
    "\n",
    "\n",
    "\n",
    "# adding timeframe \n",
    "before = int(dt.datetime(2022,1,2,0,0).timestamp())\n",
    "after = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, before=before, after=after, q='climate+change')\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4abb",
   "metadata": {},
   "source": [
    "## Start scraping\n",
    "Scraping the above selected subreddit for every day. Then saving it in a csv for per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd459ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 1000 comments in year 2022\n",
      "more than 1000 comments in month 1 - 2022\n",
      "more than 1000 comments on day 1-1-2022 -- 2-1-2022\n",
      "attempting 12-hourly scrape...\n",
      "more than 1000 comments: failed 12-hourly scrape, attempting hourly scrape...\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:20:00 date after 2022-01-02 00:40:00\n",
      "Retrieved 385 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:40:00 date after 2022-01-02 00:59:00\n",
      "Retrieved 332 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:20:00 date after 2022-01-02 01:40:00\n",
      "Retrieved 384 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:40:00 date after 2022-01-02 01:59:00\n",
      "Retrieved 324 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:20:00 date after 2022-01-02 02:40:00\n",
      "Retrieved 371 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:40:00 date after 2022-01-02 02:59:00\n",
      "Retrieved 312 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "datebefore 2022-01-02 04:00:00 date after 2022-01-02 03:00:00\n",
      "Retrieved 989 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 05:00:00 date after 2022-01-02 04:00:00\n",
      "Retrieved 836 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 06:00:00 date after 2022-01-02 05:00:00\n",
      "Retrieved 813 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 07:00:00 date after 2022-01-02 06:00:00\n",
      "Retrieved 731 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 08:00:00 date after 2022-01-02 07:00:00\n",
      "Retrieved 590 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 09:00:00 date after 2022-01-02 08:00:00\n",
      "Retrieved 469 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 10:00:00 date after 2022-01-02 09:00:00\n",
      "Retrieved 370 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 11:00:00 date after 2022-01-02 10:00:00\n",
      "Retrieved 284 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 12:00:00 date after 2022-01-02 11:00:00\n",
      "Retrieved 300 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 13:00:00 date after 2022-01-02 12:00:00\n",
      "Retrieved 354 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 14:00:00 date after 2022-01-02 13:00:00\n",
      "Retrieved 376 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 15:00:00 date after 2022-01-02 14:00:00\n",
      "Retrieved 575 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 16:00:00 date after 2022-01-02 15:00:00\n",
      "Retrieved 752 comments per hour from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:20:00 date after 2022-01-02 16:40:00\n",
      "Retrieved 462 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:40:00 date after 2022-01-02 16:59:00\n",
      "Retrieved 448 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:20:00 date after 2022-01-02 17:40:00\n",
      "Retrieved 518 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:40:00 date after 2022-01-02 17:59:00\n",
      "Retrieved 521 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:20:00 date after 2022-01-02 18:40:00\n",
      "Retrieved 938 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:40:00 date after 2022-01-02 18:59:00\n",
      "Retrieved 786 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:20:00 date after 2022-01-02 19:40:00\n",
      "Retrieved 704 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:40:00 date after 2022-01-02 19:59:00\n",
      "Retrieved 627 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:20:00 date after 2022-01-02 20:40:00\n",
      "Retrieved 531 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:40:00 date after 2022-01-02 20:59:00\n",
      "Retrieved 527 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:20:00 date after 2022-01-02 21:40:00\n",
      "Retrieved 505 comments per 20 minutes from Pushshift\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateafter 2022-01-02 21:40:00 date after 2022-01-02 21:59:00\n",
      "Retrieved 465 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:20:00 date after 2022-01-02 22:40:00\n",
      "Retrieved 452 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:40:00 date after 2022-01-02 22:59:00\n",
      "Retrieved 376 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:20:00 date after 2022-01-02 23:40:00\n",
      "Retrieved 322 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:40:00 date after 2022-01-02 23:59:00\n",
      "Retrieved 339 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "more than 1000 comments: failed 12-hourly scrape, attempting hourly scrape...\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:20:00 date after 2022-01-02 00:40:00\n",
      "Retrieved 385 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:40:00 date after 2022-01-02 00:59:00\n",
      "Retrieved 332 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 00:00:00 date after 2022-01-02 00:20:00\n",
      "Retrieved 377 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:20:00 date after 2022-01-02 01:40:00\n",
      "Retrieved 384 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:40:00 date after 2022-01-02 01:59:00\n",
      "Retrieved 324 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 01:00:00 date after 2022-01-02 01:20:00\n",
      "Retrieved 338 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:20:00 date after 2022-01-02 02:40:00\n",
      "Retrieved 371 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:40:00 date after 2022-01-02 02:59:00\n",
      "Retrieved 312 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 02:00:00 date after 2022-01-02 02:20:00\n",
      "Retrieved 398 comments per 20 minutes from Pushshift\n",
      "datebefore 2022-01-02 04:00:00 date after 2022-01-02 03:00:00\n",
      "Retrieved 989 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 05:00:00 date after 2022-01-02 04:00:00\n",
      "Retrieved 836 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 06:00:00 date after 2022-01-02 05:00:00\n",
      "Retrieved 813 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 07:00:00 date after 2022-01-02 06:00:00\n",
      "Retrieved 731 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 08:00:00 date after 2022-01-02 07:00:00\n",
      "Retrieved 590 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 09:00:00 date after 2022-01-02 08:00:00\n",
      "Retrieved 469 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 10:00:00 date after 2022-01-02 09:00:00\n",
      "Retrieved 370 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 11:00:00 date after 2022-01-02 10:00:00\n",
      "Retrieved 284 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 12:00:00 date after 2022-01-02 11:00:00\n",
      "Retrieved 300 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 13:00:00 date after 2022-01-02 12:00:00\n",
      "Retrieved 354 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 14:00:00 date after 2022-01-02 13:00:00\n",
      "Retrieved 376 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 15:00:00 date after 2022-01-02 14:00:00\n",
      "Retrieved 575 comments per hour from Pushshift\n",
      "datebefore 2022-01-02 16:00:00 date after 2022-01-02 15:00:00\n",
      "Retrieved 752 comments per hour from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:20:00 date after 2022-01-02 16:40:00\n",
      "Retrieved 462 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:40:00 date after 2022-01-02 16:59:00\n",
      "Retrieved 448 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 16:00:00 date after 2022-01-02 16:20:00\n",
      "Retrieved 334 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:20:00 date after 2022-01-02 17:40:00\n",
      "Retrieved 518 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:40:00 date after 2022-01-02 17:59:00\n",
      "Retrieved 521 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 17:00:00 date after 2022-01-02 17:20:00\n",
      "Retrieved 491 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:20:00 date after 2022-01-02 18:40:00\n",
      "Retrieved 938 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:40:00 date after 2022-01-02 18:59:00\n",
      "Retrieved 786 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 18:00:00 date after 2022-01-02 18:20:00\n",
      "Retrieved 790 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:20:00 date after 2022-01-02 19:40:00\n",
      "Retrieved 704 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:40:00 date after 2022-01-02 19:59:00\n",
      "Retrieved 627 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 19:00:00 date after 2022-01-02 19:20:00\n",
      "Retrieved 692 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateafter 2022-01-02 20:20:00 date after 2022-01-02 20:40:00\n",
      "Retrieved 0 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:40:00 date after 2022-01-02 20:59:00\n",
      "Retrieved 527 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 20:00:00 date after 2022-01-02 20:20:00\n",
      "Retrieved 538 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:20:00 date after 2022-01-02 21:40:00\n",
      "Retrieved 505 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:40:00 date after 2022-01-02 21:59:00\n",
      "Retrieved 465 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 21:00:00 date after 2022-01-02 21:20:00\n",
      "Retrieved 507 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:20:00 date after 2022-01-02 22:40:00\n",
      "Retrieved 452 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:40:00 date after 2022-01-02 22:59:00\n",
      "Retrieved 376 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 22:00:00 date after 2022-01-02 22:20:00\n",
      "Retrieved 383 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:20:00 date after 2022-01-02 23:40:00\n",
      "Retrieved 322 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:40:00 date after 2022-01-02 23:59:00\n",
      "Retrieved 339 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-02 23:00:00 date after 2022-01-02 23:20:00\n",
      "Retrieved 343 comments per 20 minutes from Pushshift\n",
      "more than 1000 comments on day 2-1-2022 -- 3-1-2022\n",
      "attempting 12-hourly scrape...\n",
      "more than 1000 comments: failed 12-hourly scrape, attempting hourly scrape...\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-03 00:00:00 date after 2022-01-03 00:20:00\n",
      "Retrieved 365 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 00:20:00 date after 2022-01-03 00:40:00\n",
      "Retrieved 393 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 00:40:00 date after 2022-01-03 00:59:00\n",
      "Retrieved 332 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 00:00:00 date after 2022-01-03 00:20:00\n",
      "Retrieved 365 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 00:00:00 date after 2022-01-03 00:20:00\n",
      "Retrieved 365 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 00:00:00 date after 2022-01-03 00:20:00\n",
      "Retrieved 365 comments per 20 minutes from Pushshift\n",
      "datebefore 2022-01-03 02:00:00 date after 2022-01-03 01:00:00\n",
      "Retrieved 875 comments per hour from Pushshift\n",
      "datebefore 2022-01-03 03:00:00 date after 2022-01-03 02:00:00\n",
      "Retrieved 877 comments per hour from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-03 03:00:00 date after 2022-01-03 03:20:00\n",
      "Retrieved 364 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 03:20:00 date after 2022-01-03 03:40:00\n",
      "Retrieved 374 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 03:40:00 date after 2022-01-03 03:59:00\n",
      "Retrieved 368 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 03:00:00 date after 2022-01-03 03:20:00\n",
      "Retrieved 364 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 03:00:00 date after 2022-01-03 03:20:00\n",
      "Retrieved 364 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 03:00:00 date after 2022-01-03 03:20:00\n",
      "Retrieved 364 comments per 20 minutes from Pushshift\n",
      "more than 1000 per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "dateafter 2022-01-03 04:00:00 date after 2022-01-03 04:20:00\n",
      "Retrieved 406 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 04:20:00 date after 2022-01-03 04:40:00\n",
      "Retrieved 317 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 04:40:00 date after 2022-01-03 04:59:00\n",
      "Retrieved 265 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 04:00:00 date after 2022-01-03 04:20:00\n",
      "Retrieved 406 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 04:00:00 date after 2022-01-03 04:20:00\n",
      "Retrieved 406 comments per 20 minutes from Pushshift\n",
      "dateafter 2022-01-03 04:00:00 date after 2022-01-03 04:20:00\n",
      "Retrieved 406 comments per 20 minutes from Pushshift\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "for m in range(9, 13):\n",
    "    year = 2022\n",
    "    yearAfter = year\n",
    "    monthBefore = m\n",
    "    monthAfter = m\n",
    "    day = 32\n",
    "    minutes = 0\n",
    "    \n",
    "    # YEARLY SCRAPE\n",
    "    # If year has comments < 1000 scrape everything at once \n",
    "    scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "    scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "       \n",
    "    # Scrape all comments per Year\n",
    "    commentsYear = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "    \n",
    "    \n",
    "    if (len(commentsYear) < 1000): \n",
    "        print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "        # convert comments into a datafrom\n",
    "        comments_df = pd.DataFrame(commentsYear)\n",
    "        \n",
    "        # Filter on keywords Climate and Change\n",
    "        for index, row in comments_df.iterrows():\n",
    "            text = row['body']\n",
    "            regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "            regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "            if regex_climate.search(text) and regex_change.search(text):\n",
    "                # if the string contains both 'Climate' and 'Change',\n",
    "                # add the row to the new dataframe\n",
    "                temp_df = temp_df.append(row, ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # MONTLY SCRAPE\n",
    "    else:\n",
    "        print(f'more than {len(commentsYear)} comments in year {year}')\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per day\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            # Filter on keywords Climate and Change\n",
    "            for index, row in comments_df.iterrows():\n",
    "                text = row['body']\n",
    "                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                if regex_climate.search(text) and regex_change.search(text):\n",
    "                    # if the string contains both 'Climate' and 'Change',\n",
    "                    # add the row to the new dataframe\n",
    "                    temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    # Filter on keywords Climate and Change\n",
    "                    for index, row in comments_df.iterrows():\n",
    "                        text = row['body']\n",
    "                        regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                        regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                        if regex_climate.search(text) and regex_change.search(text):\n",
    "                            # if the string contains both 'Climate' and 'Change',\n",
    "                            # add the row to the new dataframe\n",
    "                            temp_df = temp_df.append(row, ignore_index=True)\n",
    "      \n",
    "                # 12 HOURLY SCRAPE\n",
    "                else:\n",
    "                    print(f'more than {len(commentsDay)} comments on day {dayAfter}-{monthAfter}-{yearAfter} -- {dayBefore}-{monthBefore}-{year}')\n",
    "                    print(f'attempting 12-hourly scrape...')\n",
    "                    for b in range(2):\n",
    "\n",
    "                        hourMeasureBefore = 12\n",
    "                        hourMeasureAfter = 0\n",
    "                        minuteMeasureBefore = 0\n",
    "                        \n",
    "                        if (b == 0):\n",
    "                            hourMeasureBefore = 12\n",
    "                            hourMeasureAfter = 0\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        if (b == 1):\n",
    "                            hourMeasureBefore = 23\n",
    "                            hourMeasureAfter = 12\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        \n",
    "                        scrapeTwelveHourDateBefore = int(dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore).timestamp())\n",
    "                        scrapeTwelveHourDateAfter = int(dt.datetime(year,m,d,hourMeasureAfter,0).timestamp())  \n",
    "\n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsTwelveHour = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeTwelveHourDateBefore, after=scrapeTwelveHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsTwelveHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore), 'date after', dt.datetime(yearAfter,m,d,hourMeasureAfter,0)) \n",
    "                            print(f'Retrieved {len(commentsTwelveHour)} comments per 12 hours from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsTwelveHour)\n",
    "\n",
    "                            # Filter on keywords Climate and Change\n",
    "                            for index, row in comments_df.iterrows():\n",
    "                                text = row['body']\n",
    "                                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                if regex_climate.search(text) and regex_change.search(text):\n",
    "                                    # if the string contains both 'Climate' and 'Change',\n",
    "                                    # add the row to the new dataframe\n",
    "                                    temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                        # HOURLY SCRAPE\n",
    "                        else: \n",
    "                            print(f'more than {len(commentsTwelveHour)} comments: failed 12-hourly scrape, attempting hourly scrape...')\n",
    "                            \n",
    "                            for h in range(24):\n",
    "\n",
    "                                # add an hour to the datetime object\n",
    "                                hourBefore = h + 1\n",
    "                                hourAfter = h\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (hourBefore == 24):\n",
    "                                    hourBefore = 23\n",
    "                                    minutes = 59\n",
    "                                else: \n",
    "                                    hourBefore = h + 1\n",
    "                                    minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                                #set time frame for scraping -> Scraping for every hour in the day\n",
    "                                scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                                scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                                # Scrape all comments per hour\n",
    "                                commentsHour = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsHour) < 1000): \n",
    "                                    print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                                    print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                                    # Filter on keywords Climate and Change\n",
    "                                    for index, row in comments_df.iterrows():\n",
    "                                        text = row['body']\n",
    "                                        regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                        regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                        if regex_climate.search(text) and regex_change.search(text):\n",
    "                                            # if the string contains both 'Climate' and 'Change',\n",
    "                                            # add the row to the new dataframe\n",
    "                                            temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "                                            \n",
    "                                            \n",
    "                                # SCRAPING PER 20 MINUTES\n",
    "                                else:\n",
    "                                    print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "                                \n",
    "                                    for q in range(3):\n",
    "\n",
    "                                        minuteBefore = 20\n",
    "                                        minuteAfter = 0\n",
    "\n",
    "                                        if (q == 0):\n",
    "                                            minuteBefore = 20\n",
    "                                            minuteAfter = 0\n",
    "\n",
    "                                        if (q == 1):\n",
    "                                            minuteBefore = 40\n",
    "                                            minuteAfter = 20\n",
    "                                            \n",
    "                                        if (q == 2):\n",
    "                                            minuteBefore = 59\n",
    "                                            minuteAfter = 40\n",
    "\n",
    "\n",
    "                                        scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                        scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                        # Scrape all comments per hour\n",
    "                                        commentsMinute = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                        # if statement hour comments < 1000\n",
    "                                        if (len(commentsMinute) < 1000): \n",
    "                                            print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minuteBefore)) \n",
    "                                            print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                            # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            # Filter on keywords Climate and Change\n",
    "                                            for index, row in comments_df.iterrows():\n",
    "                                                text = row['body']\n",
    "                                                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                                if regex_climate.search(text) and regex_change.search(text):\n",
    "                                                    # if the string contains both 'Climate' and 'Change',\n",
    "                                                    # add the row to the new dataframe\n",
    "                                                    temp_df = temp_df.append(row, ignore_index=True)                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                        else:\n",
    "                                        # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            # Filter on keywords Climate and Change\n",
    "                                            for index, row in comments_df.iterrows():\n",
    "                                                text = row['body']\n",
    "                                                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                                if regex_climate.search(text) and regex_change.search(text):\n",
    "                                                    # if the string contains both 'Climate' and 'Change',\n",
    "                                                    # add the row to the new dataframe\n",
    "                                                    temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "                                            print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                            print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minuteAfter} and {h}:{minuteBefore}')\n",
    "                #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
