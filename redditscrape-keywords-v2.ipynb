{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce1c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca79d5",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ddba",
   "metadata": {},
   "source": [
    "## select subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38949ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 12 comments from Pushshift science\n"
     ]
    }
   ],
   "source": [
    "subreddit=\"science\"\n",
    "limit=1000\n",
    "\n",
    "\n",
    "\n",
    "# adding timeframe \n",
    "before = int(dt.datetime(2022,1,2,0,0).timestamp())\n",
    "after = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift {subreddit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4abb",
   "metadata": {},
   "source": [
    "## Start scraping\n",
    "Scraping the above selected subreddit for every day. Then saving it in a csv for per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd459ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more thann 1000 comments in 2017 in sr: science. Attempting monthyly scraping...\n",
      "Retrieved 862 comments in month: 1 - from Pushshift\n",
      "Retrieved 683 comments in month: 2 - from Pushshift\n",
      "Retrieved 523 comments in month: 3 - from Pushshift\n",
      "Retrieved 508 comments in month: 4 - from Pushshift\n",
      "Retrieved 689 comments in month: 5 - from Pushshift\n",
      "more than 1000 comments in month 6 - 2017\n",
      "datebefore 1 - 6 -  2017 date after 31 - 5 -  2017\n",
      "Retrieved 15 comments per day from Pushshift\n",
      "datebefore 2 - 6 -  2017 date after 1 - 6 -  2017\n",
      "Retrieved 11 comments per day from Pushshift\n",
      "datebefore 3 - 6 -  2017 date after 2 - 6 -  2017\n",
      "Retrieved 747 comments per day from Pushshift\n",
      "datebefore 4 - 6 -  2017 date after 3 - 6 -  2017\n",
      "Retrieved 31 comments per day from Pushshift\n",
      "datebefore 5 - 6 -  2017 date after 4 - 6 -  2017\n",
      "Retrieved 12 comments per day from Pushshift\n",
      "datebefore 6 - 6 -  2017 date after 5 - 6 -  2017\n",
      "Retrieved 33 comments per day from Pushshift\n",
      "datebefore 7 - 6 -  2017 date after 6 - 6 -  2017\n",
      "Retrieved 61 comments per day from Pushshift\n",
      "datebefore 8 - 6 -  2017 date after 7 - 6 -  2017\n",
      "Retrieved 53 comments per day from Pushshift\n",
      "datebefore 9 - 6 -  2017 date after 8 - 6 -  2017\n",
      "Retrieved 15 comments per day from Pushshift\n",
      "datebefore 10 - 6 -  2017 date after 9 - 6 -  2017\n",
      "Retrieved 6 comments per day from Pushshift\n",
      "datebefore 11 - 6 -  2017 date after 10 - 6 -  2017\n",
      "Retrieved 14 comments per day from Pushshift\n",
      "datebefore 12 - 6 -  2017 date after 11 - 6 -  2017\n",
      "Retrieved 16 comments per day from Pushshift\n",
      "datebefore 13 - 6 -  2017 date after 12 - 6 -  2017\n",
      "Retrieved 23 comments per day from Pushshift\n",
      "datebefore 14 - 6 -  2017 date after 13 - 6 -  2017\n",
      "Retrieved 3 comments per day from Pushshift\n",
      "datebefore 15 - 6 -  2017 date after 14 - 6 -  2017\n",
      "Retrieved 4 comments per day from Pushshift\n",
      "datebefore 16 - 6 -  2017 date after 15 - 6 -  2017\n",
      "Retrieved 8 comments per day from Pushshift\n",
      "datebefore 17 - 6 -  2017 date after 16 - 6 -  2017\n",
      "Retrieved 4 comments per day from Pushshift\n",
      "datebefore 18 - 6 -  2017 date after 17 - 6 -  2017\n",
      "Retrieved 0 comments per day from Pushshift\n",
      "datebefore 19 - 6 -  2017 date after 18 - 6 -  2017\n",
      "Retrieved 4 comments per day from Pushshift\n",
      "datebefore 20 - 6 -  2017 date after 19 - 6 -  2017\n",
      "Retrieved 21 comments per day from Pushshift\n",
      "datebefore 21 - 6 -  2017 date after 20 - 6 -  2017\n",
      "Retrieved 58 comments per day from Pushshift\n",
      "datebefore 22 - 6 -  2017 date after 21 - 6 -  2017\n",
      "Retrieved 4 comments per day from Pushshift\n",
      "datebefore 23 - 6 -  2017 date after 22 - 6 -  2017\n",
      "Retrieved 116 comments per day from Pushshift\n",
      "datebefore 24 - 6 -  2017 date after 23 - 6 -  2017\n",
      "Retrieved 11 comments per day from Pushshift\n",
      "datebefore 25 - 6 -  2017 date after 24 - 6 -  2017\n",
      "Retrieved 1 comments per day from Pushshift\n",
      "datebefore 26 - 6 -  2017 date after 25 - 6 -  2017\n",
      "Retrieved 4 comments per day from Pushshift\n",
      "datebefore 27 - 6 -  2017 date after 26 - 6 -  2017\n",
      "Retrieved 23 comments per day from Pushshift\n",
      "datebefore 28 - 6 -  2017 date after 27 - 6 -  2017\n",
      "Retrieved 62 comments per day from Pushshift\n",
      "datebefore 29 - 6 -  2017 date after 28 - 6 -  2017\n",
      "Retrieved 35 comments per day from Pushshift\n",
      "datebefore 30 - 6 -  2017 date after 29 - 6 -  2017\n",
      "Retrieved 19 comments per day from Pushshift\n",
      "Retrieved 429 comments in month: 7 - from Pushshift\n",
      "Retrieved 370 comments in month: 8 - from Pushshift\n",
      "Retrieved 239 comments in month: 9 - from Pushshift\n",
      "Retrieved 166 comments in month: 10 - from Pushshift\n",
      "Retrieved 579 comments in month: 11 - from Pushshift\n",
      "Retrieved 270 comments in month: 12 - from Pushshift\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "year = 2017\n",
    "\n",
    "# YEARLY SCRAPE\n",
    "# If year has comments < 1000 scrape everything at once \n",
    "scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "\n",
    "# Scrape all comments per Year\n",
    "commentsYear = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "\n",
    "\n",
    "if (len(commentsYear) > 0 and len(commentsYear) < 1000): \n",
    "    print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "    # convert comments into a datafrom\n",
    "    comments_df = pd.DataFrame(commentsYear)\n",
    "\n",
    "    #add all datapoints to a temporary dataframe\n",
    "    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# MONTLY SCRAPE\n",
    "else:\n",
    "\n",
    "    print(f'more thann {len(commentsYear)} comments in {year} in sr: {subreddit}. Attempting monthyly scraping...')\n",
    "    \n",
    "    for m in range(1, 13):\n",
    "        yearAfter = year\n",
    "        monthBefore = m\n",
    "        monthAfter = m\n",
    "        day = 32\n",
    "        minutes = 0\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per month\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) > 0 and len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            #add all datapoints to a temporary dataframe\n",
    "            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    #add all datapoints to a temporary dataframe\n",
    "                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "                # 12 HOURLY SCRAPE\n",
    "                else:\n",
    "                    print(f'more than {len(commentsDay)} comments on day {dayAfter}-{monthAfter}-{yearAfter} -- {dayBefore}-{monthBefore}-{year}')\n",
    "                    print(f'attempting 12-hourly scrape...')\n",
    "                    for b in range(2):\n",
    "\n",
    "                        hourMeasureBefore = 12\n",
    "                        hourMeasureAfter = 0\n",
    "                        minuteMeasureBefore = 0\n",
    "                        \n",
    "                        if (b == 0):\n",
    "                            hourMeasureBefore = 12\n",
    "                            hourMeasureAfter = 0\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        if (b == 1):\n",
    "                            hourMeasureBefore = 23\n",
    "                            hourMeasureAfter = 12\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        \n",
    "                        scrapeTwelveHourDateBefore = int(dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore).timestamp())\n",
    "                        scrapeTwelveHourDateAfter = int(dt.datetime(year,m,d,hourMeasureAfter,0).timestamp())  \n",
    "\n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsTwelveHour = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeTwelveHourDateBefore, after=scrapeTwelveHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsTwelveHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore), 'date after', dt.datetime(yearAfter,m,d,hourMeasureAfter,0)) \n",
    "                            print(f'Retrieved {len(commentsTwelveHour)} comments per 12 hours from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsTwelveHour)\n",
    "\n",
    "                            #add all datapoints to a temporary dataframe\n",
    "                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                        # HOURLY SCRAPE\n",
    "                        else: \n",
    "                            print(f'more than {len(commentsTwelveHour)} comments: failed 12-hourly scrape, attempting hourly scrape...')\n",
    "                            \n",
    "                            for h in range(24):\n",
    "\n",
    "                                # add an hour to the datetime object\n",
    "                                hourBefore = h + 1\n",
    "                                hourAfter = h\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (hourBefore == 24):\n",
    "                                    hourBefore = 23\n",
    "                                    minutes = 59\n",
    "                                else: \n",
    "                                    hourBefore = h + 1\n",
    "                                    minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                                #set time frame for scraping -> Scraping for every hour in the day\n",
    "                                scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                                scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                                # Scrape all comments per hour\n",
    "                                commentsHour = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsHour) < 1000): \n",
    "                                    print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                                    print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "                                            \n",
    "                                            \n",
    "                                # SCRAPING PER 20 MINUTES\n",
    "                                else:\n",
    "                                    print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "                                \n",
    "                                    for q in range(3):\n",
    "\n",
    "                                        minuteBefore = 20\n",
    "                                        minuteAfter = 0\n",
    "\n",
    "                                        if (q == 0):\n",
    "                                            minuteBefore = 20\n",
    "                                            minuteAfter = 0\n",
    "\n",
    "                                        if (q == 1):\n",
    "                                            minuteBefore = 40\n",
    "                                            minuteAfter = 20\n",
    "                                            \n",
    "                                        if (q == 2):\n",
    "                                            minuteBefore = 59\n",
    "                                            minuteAfter = 40\n",
    "\n",
    "\n",
    "                                        scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                        scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                        # Scrape all comments per 20 mins\n",
    "                                        commentsMinute = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                        # if statement hour comments < 1000\n",
    "                                        if (len(commentsMinute) < 1000): \n",
    "                                            print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minuteBefore)) \n",
    "                                            print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                            # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            #add all datapoints to a temporary dataframe\n",
    "                                            temp_df = temp_df.append(comments_df, ignore_index=True)                                   \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                        else:\n",
    "                                        # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            #add all datapoints to a temporary dataframe\n",
    "                                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "                                            print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                            print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minuteAfter} and {h}:{minuteBefore}')\n",
    "                #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5acfd091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5260\n"
     ]
    }
   ],
   "source": [
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cadfbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more thann 1000 comments in 2015 in sr: science. Attempting monthyly scraping...\n",
      "Retrieved 222 comments in month: 1 - from Pushshift\n",
      "Retrieved 50 comments in month: 2 - from Pushshift\n",
      "Retrieved 629 comments in month: 3 - from Pushshift\n",
      "Retrieved 270 comments in month: 4 - from Pushshift\n",
      "Retrieved 931 comments in month: 5 - from Pushshift\n",
      "Retrieved 284 comments in month: 6 - from Pushshift\n",
      "Retrieved 537 comments in month: 7 - from Pushshift\n",
      "Retrieved 834 comments in month: 8 - from Pushshift\n",
      "Retrieved 440 comments in month: 9 - from Pushshift\n",
      "Retrieved 342 comments in month: 10 - from Pushshift\n",
      "Retrieved 471 comments in month: 11 - from Pushshift\n",
      "Retrieved 184 comments in month: 12 - from Pushshift\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "year = 2016\n",
    "\n",
    "# YEARLY SCRAPE\n",
    "# If year has comments < 1000 scrape everything at once \n",
    "scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "\n",
    "# Scrape all comments per Year\n",
    "commentsYear = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "\n",
    "\n",
    "if (len(commentsYear) > 0 and len(commentsYear) < 1000): \n",
    "    print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "    # convert comments into a datafrom\n",
    "    comments_df = pd.DataFrame(commentsYear)\n",
    "\n",
    "    #add all datapoints to a temporary dataframe\n",
    "    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# MONTLY SCRAPE\n",
    "else:\n",
    "\n",
    "    print(f'more thann {len(commentsYear)} comments in {year} in sr: {subreddit}. Attempting monthyly scraping...')\n",
    "    \n",
    "    for m in range(1, 13):\n",
    "        yearAfter = year\n",
    "        monthBefore = m\n",
    "        monthAfter = m\n",
    "        day = 32\n",
    "        minutes = 0\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per month\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) > 0 and len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            #add all datapoints to a temporary dataframe\n",
    "            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    #add all datapoints to a temporary dataframe\n",
    "                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "                # 12 HOURLY SCRAPE\n",
    "                else:\n",
    "                    print(f'more than {len(commentsDay)} comments on day {dayAfter}-{monthAfter}-{yearAfter} -- {dayBefore}-{monthBefore}-{year}')\n",
    "                    print(f'attempting 12-hourly scrape...')\n",
    "                    for b in range(2):\n",
    "\n",
    "                        hourMeasureBefore = 12\n",
    "                        hourMeasureAfter = 0\n",
    "                        minuteMeasureBefore = 0\n",
    "                        \n",
    "                        if (b == 0):\n",
    "                            hourMeasureBefore = 12\n",
    "                            hourMeasureAfter = 0\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        if (b == 1):\n",
    "                            hourMeasureBefore = 23\n",
    "                            hourMeasureAfter = 12\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        \n",
    "                        scrapeTwelveHourDateBefore = int(dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore).timestamp())\n",
    "                        scrapeTwelveHourDateAfter = int(dt.datetime(year,m,d,hourMeasureAfter,0).timestamp())  \n",
    "\n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsTwelveHour = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeTwelveHourDateBefore, after=scrapeTwelveHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsTwelveHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore), 'date after', dt.datetime(yearAfter,m,d,hourMeasureAfter,0)) \n",
    "                            print(f'Retrieved {len(commentsTwelveHour)} comments per 12 hours from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsTwelveHour)\n",
    "\n",
    "                            #add all datapoints to a temporary dataframe\n",
    "                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                        # HOURLY SCRAPE\n",
    "                        else: \n",
    "                            print(f'more than {len(commentsTwelveHour)} comments: failed 12-hourly scrape, attempting hourly scrape...')\n",
    "                            \n",
    "                            for h in range(24):\n",
    "\n",
    "                                # add an hour to the datetime object\n",
    "                                hourBefore = h + 1\n",
    "                                hourAfter = h\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (hourBefore == 24):\n",
    "                                    hourBefore = 23\n",
    "                                    minutes = 59\n",
    "                                else: \n",
    "                                    hourBefore = h + 1\n",
    "                                    minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                                #set time frame for scraping -> Scraping for every hour in the day\n",
    "                                scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                                scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                                # Scrape all comments per hour\n",
    "                                commentsHour = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsHour) < 1000): \n",
    "                                    print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                                    print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                                    #add all datapoints to a temporary dataframe\n",
    "                                    temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "                                            \n",
    "                                            \n",
    "                                # SCRAPING PER 20 MINUTES\n",
    "                                else:\n",
    "                                    print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "                                \n",
    "                                    for q in range(3):\n",
    "\n",
    "                                        minuteBefore = 20\n",
    "                                        minuteAfter = 0\n",
    "\n",
    "                                        if (q == 0):\n",
    "                                            minuteBefore = 20\n",
    "                                            minuteAfter = 0\n",
    "\n",
    "                                        if (q == 1):\n",
    "                                            minuteBefore = 40\n",
    "                                            minuteAfter = 20\n",
    "                                            \n",
    "                                        if (q == 2):\n",
    "                                            minuteBefore = 59\n",
    "                                            minuteAfter = 40\n",
    "\n",
    "\n",
    "                                        scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                        scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                        # Scrape all comments per 20 mins\n",
    "                                        commentsMinute = api.search_comments(subreddit=subreddit, q='climate+change', limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                        # if statement hour comments < 1000\n",
    "                                        if (len(commentsMinute) < 1000): \n",
    "                                            print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minuteBefore)) \n",
    "                                            print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                            # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            #add all datapoints to a temporary dataframe\n",
    "                                            temp_df = temp_df.append(comments_df, ignore_index=True)                                   \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                        else:\n",
    "                                        # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            #add all datapoints to a temporary dataframe\n",
    "                                            temp_df = temp_df.append(comments_df, ignore_index=True)\n",
    "\n",
    "                                            print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                            print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minuteAfter} and {h}:{minuteBefore}')\n",
    "                #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ca177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
