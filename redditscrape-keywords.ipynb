{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce1c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca79d5",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6558268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ddba",
   "metadata": {},
   "source": [
    "## select subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38949ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1000 comments from Pushshift\n"
     ]
    }
   ],
   "source": [
    "subreddit=\"politics\"\n",
    "limit=1000\n",
    "\n",
    "# adding timeframe \n",
    "before = int(dt.datetime(2022,1,1,1,0).timestamp())\n",
    "after = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf4abb",
   "metadata": {},
   "source": [
    "## Start scraping\n",
    "Scraping the above selected subreddit for every day. Then saving it in a csv for per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd459ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 1000 comments in year 2020\n",
      "more than 1000 comments in month 1 - 2020\n",
      "more than 1000 comments on day 1-1-2020 -- 2-1-2020\n",
      "attempting 12-hourly scrape...\n",
      "failed 12-hourly scrape, attempting hourly scrape...\n",
      "more than 1000 comments on day 1-1-2020 -- 2-1-2020\n",
      "failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "more than 1000 comments in hours 2-1-2020 0:0 -- 1:0\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 994 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 857 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 768 comments per 12 hours from Pushshift\n",
      "failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "more than 1000 comments in hours 2-1-2020 1:0 -- 2:0\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 766 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 803 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 775 comments per 12 hours from Pushshift\n",
      "failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "more than 1000 comments in hours 2-1-2020 2:0 -- 3:0\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 768 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 791 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 719 comments per 12 hours from Pushshift\n",
      "failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "more than 1000 comments in hours 2-1-2020 3:0 -- 4:0\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 742 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 631 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 603 comments per 12 hours from Pushshift\n",
      "failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "more than 1000 comments in hours 2-1-2020 4:0 -- 5:0\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 643 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 638 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 574 comments per 12 hours from Pushshift\n",
      "failed to do an hourly scrape, attempting 20 minute scrapes scrape...\n",
      "more than 1000 comments in hours 2-1-2020 5:0 -- 6:0\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 503 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 480 comments per 12 hours from Pushshift\n",
      "datebefore 2020-01-02 12:00:00 date after 2020-01-02 00:00:00\n",
      "Retrieved 388 comments per 12 hours from Pushshift\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "temp_df = pd.DataFrame() #Temporary empty dataframe\n",
    "\n",
    "for m in range(1, 13):\n",
    "    year = 2020\n",
    "    yearAfter = year\n",
    "    monthBefore = m\n",
    "    monthAfter = m\n",
    "    day = 32\n",
    "    minutes = 0\n",
    "    \n",
    "    # YEARLY SCRAPE\n",
    "    # If year has comments < 1000 scrape everything at once \n",
    "    scrapeYearDateBefore = int(dt.datetime(year,12,31,0,0).timestamp())\n",
    "    scrapeYearDateAfter = int(dt.datetime(year,1,1,0,0).timestamp())\n",
    "       \n",
    "    # Scrape all comments per Year\n",
    "    commentsYear = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeYearDateBefore, after=scrapeYearDateAfter)\n",
    "    \n",
    "    \n",
    "    if (len(commentsYear) < 1000): \n",
    "        print(f'Retrieved {len(commentsYear)} comments in {year} from Pushshift')\n",
    "        # convert comments into a datafrom\n",
    "        comments_df = pd.DataFrame(commentsYear)\n",
    "        \n",
    "        # Filter on keywords Climate and Change\n",
    "        for index, row in comments_df.iterrows():\n",
    "            text = row['body']\n",
    "            regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "            regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "            if regex_climate.search(text) and regex_change.search(text):\n",
    "                # if the string contains both 'Climate' and 'Change',\n",
    "                # add the row to the new dataframe\n",
    "                temp_df = temp_df.append(row, ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # MONTLY SCRAPE\n",
    "    else:\n",
    "        print(f'more than {len(commentsYear)} comments in year {year}')\n",
    "        endMonthDate = 31\n",
    "        \n",
    "        if(m == 1) or (m == 3) or (m == 5) or (m == 7) or (m == 8) or (m == 10) or (m == 12):\n",
    "            endMonthDate = 31\n",
    "        elif(m == 2):\n",
    "            endMonthDate = 28\n",
    "        else: \n",
    "            endMonthDate = 30\n",
    "            \n",
    "        # If month has comments < 1000 scrape all monthly data at once \n",
    "        scrapeMonthDateBefore = int(dt.datetime(year,m,endMonthDate,0,0).timestamp())\n",
    "        scrapeMonthDateAfter = int(dt.datetime(year,m,1,0,0).timestamp())\n",
    "        \n",
    "        # Scrape all comments per day\n",
    "        commentsMonth = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMonthDateBefore, after=scrapeMonthDateAfter)\n",
    "        \n",
    "        if (len(commentsMonth) < 1000): \n",
    "            print(f'Retrieved {len(commentsMonth)} comments in month: {m} - from Pushshift')\n",
    "            # convert comments into a datafrom\n",
    "            comments_df = pd.DataFrame(commentsMonth)\n",
    "\n",
    "            # Filter on keywords Climate and Change\n",
    "            for index, row in comments_df.iterrows():\n",
    "                text = row['body']\n",
    "                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                if regex_climate.search(text) and regex_change.search(text):\n",
    "                    # if the string contains both 'Climate' and 'Change',\n",
    "                    # add the row to the new dataframe\n",
    "                    temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        # DAILY SCRAPE\n",
    "        else:\n",
    "            print(f'more than {len(commentsMonth)} comments in month {m} - {year}')\n",
    "            if (m == 2): #februari \n",
    "                day = 29\n",
    "            if (m == 4) or (m == 6) or (m == 9) or (m == 11): # months with 30 days -> its days +1 for the loop\n",
    "                day = 31\n",
    "\n",
    "            for d in range(1, day):\n",
    "                # date variables\n",
    "                dayBefore = d\n",
    "                dayAfter = d-1\n",
    "\n",
    "                # change dayAfter / monthAfter on certain conditions\n",
    "                if(dayAfter == 0): # \n",
    "                    monthAfter = m-1\n",
    "                    if(m == 2):\n",
    "                        dayAfter = 31\n",
    "                    elif(m==3):\n",
    "                        dayAfter = 28\n",
    "                    elif (m == 4) or (m == 6) or (m == 9) or (m == 11) or (m == 13):\n",
    "                        dayAfter = 31\n",
    "                    else: dayAfter = 30\n",
    "                else: monthAfter = m\n",
    "\n",
    "                if(monthAfter == 0):\n",
    "                    continue\n",
    "\n",
    "                #set time frame for scraping -> Scraping for every day in the year.\n",
    "                scrapeDayDateBefore = int(dt.datetime(year,monthBefore,dayBefore,0,0).timestamp())\n",
    "                scrapeDayDateAfter = int(dt.datetime(yearAfter,monthAfter,dayAfter,0,0).timestamp())\n",
    "\n",
    "\n",
    "                # Scrape all comments per day\n",
    "                commentsDay = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeDayDateBefore, after=scrapeDayDateAfter)\n",
    "                  \n",
    "                # if statement day comments < 1000\n",
    "                if (len(commentsDay) < 1000): \n",
    "                    print('datebefore', dayBefore, '-', monthBefore, '- ', year, 'date after', dayAfter, '-', monthAfter, '- ', yearAfter)\n",
    "                    print(f'Retrieved {len(commentsDay)} comments per day from Pushshift')\n",
    "                    # convert comments into a datafrom\n",
    "                    comments_df = pd.DataFrame(commentsDay)\n",
    "\n",
    "                    # Filter on keywords Climate and Change\n",
    "                    for index, row in comments_df.iterrows():\n",
    "                        text = row['body']\n",
    "                        regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                        regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                        if regex_climate.search(text) and regex_change.search(text):\n",
    "                            # if the string contains both 'Climate' and 'Change',\n",
    "                            # add the row to the new dataframe\n",
    "                            temp_df = temp_df.append(row, ignore_index=True)\n",
    "      \n",
    "                # 12 HOURLY SCRAPE\n",
    "                else:\n",
    "                    print(f'more than {len(commentsDay)} comments on day {dayAfter}-{monthAfter}-{yearAfter} -- {dayBefore}-{monthBefore}-{year}')\n",
    "                    print(f'attempting 12-hourly scrape...')\n",
    "                    for b in range(2):\n",
    "\n",
    "                        hourMeasureBefore = 12\n",
    "                        hourMeasureAfter = 0\n",
    "                        minuteMeasureBefore = 0\n",
    "                        \n",
    "                        if (b == 0):\n",
    "                            hourMeasureBefore = 12\n",
    "                            hourMeasureAfter = 0\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        if (b == 1):\n",
    "                            hourMeasureBefore = 23\n",
    "                            hourMeasureAfter = 12\n",
    "                            minuteMeasureBefore = 0\n",
    "\n",
    "                        \n",
    "                        scrapeTwelveHourDateBefore = int(dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore).timestamp())\n",
    "                        scrapeTwelveHourDateAfter = int(dt.datetime(year,m,d,hourMeasureAfter,0).timestamp())  \n",
    "\n",
    "\n",
    "                        # Scrape all comments per hour\n",
    "                        commentsTwelveHour = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeTwelveHourDateBefore, after=scrapeTwelveHourDateAfter)\n",
    "\n",
    "                        # if statement hour comments < 1000\n",
    "                        if (len(commentsTwelveHour) < 1000): \n",
    "                            print('datebefore', dt.datetime(year,m,d,hourMeasureBefore,minuteMeasureBefore), 'date after', dt.datetime(yearAfter,m,d,hourMeasureAfter,0)) \n",
    "                            print(f'Retrieved {len(commentsTwelveHour)} comments per 12 hours from Pushshift')\n",
    "                            # convert comments into a datafrom\n",
    "                            comments_df = pd.DataFrame(commentsTwelveHour)\n",
    "\n",
    "                            # Filter on keywords Climate and Change\n",
    "                            for index, row in comments_df.iterrows():\n",
    "                                text = row['body']\n",
    "                                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                if regex_climate.search(text) and regex_change.search(text):\n",
    "                                    # if the string contains both 'Climate' and 'Change',\n",
    "                                    # add the row to the new dataframe\n",
    "                                    temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                        # HOURLY SCRAPE\n",
    "                        else: \n",
    "                            print(f'more than {len(commentsTwelveHour)} comments: failed 12-hourly scrape, attempting hourly scrape...')\n",
    "                            \n",
    "                            for h in range(24):\n",
    "\n",
    "                                # add an hour to the datetime object\n",
    "                                hourBefore = h + 1\n",
    "                                hourAfter = h\n",
    "                                minuteAfter = 0\n",
    "\n",
    "                                if (hourBefore == 24):\n",
    "                                    hourBefore = 23\n",
    "                                    minutes = 59\n",
    "                                else: \n",
    "                                    hourBefore = h + 1\n",
    "                                    minutes = 0\n",
    "\n",
    "\n",
    "\n",
    "                                #set time frame for scraping -> Scraping for every hour in the day\n",
    "                                scrapeHourDateBefore = int(dt.datetime(year,m,d,hourBefore,minutes).timestamp())\n",
    "                                scrapeHourDateAfter = int(dt.datetime(yearAfter,m,d,hourAfter,minuteAfter).timestamp())      \n",
    "\n",
    "                                # Scrape all comments per hour\n",
    "                                commentsHour = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeHourDateBefore, after=scrapeHourDateAfter)\n",
    "\n",
    "                                # if statement hour comments < 1000\n",
    "                                if (len(commentsHour) < 1000): \n",
    "                                    print('datebefore', dt.datetime(year,m,d,hourBefore,minutes), 'date after', dt.datetime(yearAfter,m,d,hourAfter,minuteAfter)) \n",
    "                                    print(f'Retrieved {len(commentsHour)} comments per hour from Pushshift')\n",
    "                                    # convert comments into a datafrom\n",
    "                                    comments_df = pd.DataFrame(commentsHour)\n",
    "\n",
    "                                    # Filter on keywords Climate and Change\n",
    "                                    for index, row in comments_df.iterrows():\n",
    "                                        text = row['body']\n",
    "                                        regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                        regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                        if regex_climate.search(text) and regex_change.search(text):\n",
    "                                            # if the string contains both 'Climate' and 'Change',\n",
    "                                            # add the row to the new dataframe\n",
    "                                            temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "                                            \n",
    "                                            \n",
    "                                # SCRAPING PER 20 MINUTES\n",
    "                                else:\n",
    "                                    print(f'more than {len(commentsHour)} per hour: failed to do an hourly scrape, attempting 20 minute scrapes scrape...')\n",
    "                                \n",
    "                                    for q in range(3):\n",
    "\n",
    "                                        minuteBefore = 20\n",
    "                                        minuteAfter = 0\n",
    "\n",
    "                                        if (q == 0):\n",
    "                                            minuteBefore = 20\n",
    "                                            minuteAfter = 0\n",
    "\n",
    "                                        if (q == 1):\n",
    "                                            minuteBefore = 40\n",
    "                                            minuteAfter = 20\n",
    "                                            \n",
    "                                        if (q == 2):\n",
    "                                            minuteBefore = 59\n",
    "                                            minuteAfter = 40\n",
    "\n",
    "\n",
    "                                        scrapeMinuteDateBefore = int(dt.datetime(year,m,d,h,minuteBefore).timestamp())\n",
    "                                        scrapeMinuteDateAfter = int(dt.datetime(year,m,d,h,minuteAfter).timestamp())  \n",
    "\n",
    "\n",
    "                                        # Scrape all comments per hour\n",
    "                                        commentsMinute = api.search_comments(subreddit=subreddit, limit=limit, before=scrapeMinuteDateBefore, after=scrapeMinuteDateAfter)\n",
    "\n",
    "                                        # if statement hour comments < 1000\n",
    "                                        if (len(commentsMinute) < 1000): \n",
    "                                            print('dateafter', dt.datetime(year,m,d,h,minuteAfter), 'date after', dt.datetime(year,m,d,h,minutesBefore)) \n",
    "                                            print(f'Retrieved {len(commentsMinute)} comments per 20 minutes from Pushshift')\n",
    "                                            # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            # Filter on keywords Climate and Change\n",
    "                                            for index, row in comments_df.iterrows():\n",
    "                                                text = row['body']\n",
    "                                                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                                if regex_climate.search(text) and regex_change.search(text):\n",
    "                                                    # if the string contains both 'Climate' and 'Change',\n",
    "                                                    # add the row to the new dataframe\n",
    "                                                    temp_df = temp_df.append(row, ignore_index=True)                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                        else:\n",
    "                                        # convert comments into a datafrom\n",
    "                                            comments_df = pd.DataFrame(commentsMinute)\n",
    "\n",
    "                                            # Filter on keywords Climate and Change\n",
    "                                            for index, row in comments_df.iterrows():\n",
    "                                                text = row['body']\n",
    "                                                regex_climate = re.compile(r\"\\b[Cc]limate\")\n",
    "                                                regex_change = re.compile(r\"\\b[Cc]hange\")\n",
    "                                                if regex_climate.search(text) and regex_change.search(text):\n",
    "                                                    # if the string contains both 'Climate' and 'Change',\n",
    "                                                    # add the row to the new dataframe\n",
    "                                                    temp_df = temp_df.append(row, ignore_index=True)\n",
    "\n",
    "                                            print('ERROR MORE THAN 1000 per 20 minutes, however, still going on. INCOMPLETE')\n",
    "                                            print(f'more than {len(commentsMinute)} comments on day {d}-{m}-{year} between {h}:{minutesAfter} and {h}:{minutesBefore}')\n",
    "                #                             raise ValueError(\"More than 1000 comments in this HOUR.\")\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "\n",
    "# all datapoints in temporary dataframe to one dataframe\n",
    "df1 = pd.DataFrame(temp_df)\n",
    "                                      \n",
    "# create the filename with the variable name embedded\n",
    "filename = f'{subreddit}_{year}.csv'\n",
    "\n",
    "# save the dataframe to the file with the embedded variable name\n",
    "df1.to_csv(filename, encoding='utf-8', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
